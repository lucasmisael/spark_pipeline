from pyspark.sql import functions as f

df = spark.read.json("/Users/lucasmisael/Documents/pessoal/pdi/pipelinesAirflow/datalake/twitter_aluraonline")

tweet_df = df.select(f.explode("data").alias("tweets")).select("tweets.author_id","tweets.conversation_id", "tweets.created_at", "tweets.id", "tweets.in_reply_to_user_id" , "tweets.public_metrics.*", "tweets.text")

user_df = df.select(f.explode("includes.users").alias("usr")).select("usr.*")


##Exportando os dados e repartindo os dados em varias partições
--tweet_df.write.csv("/Users/lucasmisael/Documents/pessoal/pdi/pipelinesAirflow/datalake/export")
tweet_df.repartition(2).write.mode("overwrite").option("header", True).csv("/Users/lucasmisael/Documents/pessoal/pdi/pipelinesAirflow/datalake/export2")

##Exportando os dados e usando o coalesce para unir essas partiçoes 
tweet_df.repartition(5).coalesce(2).write.mode("overwrite").option("header", True).csv("/Users/lucasmisael/Documents/pessoal/pdi/pipelinesAirflow/datalake/export3")

#Convertendo para data 
from pyspark.sql.functions import to_date

tweet_df.groupBy(to_date("created_at")).count().show()

#exporta os arquivos criando uma pasta por data 
export_df = tweet_df.withColumn("created_date", to_date("created_at")).repartition("created_date")

export_df.write.mode("overwrite").partitionBy("created_date").json("/Users/lucasmisael/Documents/pessoal/pdi/pipelinesAirflow/datalake/export")